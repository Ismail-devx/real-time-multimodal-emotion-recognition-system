
Real-time multimodal emotion recognition system combining speech and text models with Dynamic Sliding Window tracking, confidence-weighted fusion, and acoustic-aware correction.


# üé≠ Real-Time Multimodal Emotion Recognition System  
### With Dynamic Sliding Window & Acoustic-Aware Correction

---

## üöÄ Overview

This project presents a **real-time multimodal emotion recognition system** designed to detect, interpret, and respond to user emotions using both **speech** and **text** inputs.

Unlike traditional emotion classifiers that operate on isolated predictions, this system introduces **adaptive reasoning mechanisms** to improve emotional continuity, robustness, and real-time interaction quality.

The architecture combines deep learning models with temporal context tracking, confidence-based fusion, and signal-level correction to produce more stable and human-aligned emotional interpretations.

---

## ‚ú® Key Capabilities

- üé§ **Real-time Speech Emotion Recognition** (Wav2Vec 2.0)  
- üí¨ **Text Emotion Classification** (BERT)  
- üîÅ **Confidence-Weighted Multimodal Fusion**  
- üß† **Dynamic Sliding Window (DSW)** for temporal emotion tracking  
- üéöÔ∏è **Acoustic-Aware Emotion Correction**  
- ü§ñ **Emotionally Adaptive AI Responses** (OpenChat via Ollama)  
- üîä **Emotion-Modulated Text-to-Speech Feedback**  
- üìä **Emotion Logging & Confusion Matrix Evaluation**

---

## üß† Key Innovations

### **Dynamic Sliding Window (DSW)**
A novel adaptive temporal mechanism that:

- Tracks recent emotion predictions
- Dynamically resizes based on emotional volatility
- Stabilises low-confidence outputs
- Smooths erratic classification shifts

This allows the system to model **emotional trajectories**, rather than treating each input independently.

---

### **Confidence-Weighted Fusion**
Speech and text predictions are merged using softmax confidence scores:

\[
P_{fused} = \frac{w_s P_s + w_t P_t}{w_s + w_t}
\]

Where:

- \(P_s\), \(P_t\) = model probability outputs  
- \(w_s\), \(w_t\) = confidence weights  

---

### **Acoustic-Aware Correction**
Low-confidence speech predictions are refined using:

- Pitch mean / variance  
- RMS volume  
- Speaking rate  

This signal-level reasoning improves robustness under:

‚úî Noisy input  
‚úî Monotone speech  
‚úî Ambiguous emotional delivery  

---

### **LLM-Based Emotional Response**
Final emotion predictions guide:

- Emotion-specific prompt templates  
- OpenChat response generation  
- Tone-adaptive feedback  

Enabling **emotionally coherent dialogue**, not just classification.

---

## üõ†Ô∏è Tech Stack

### **Languages**
Python

---

### **Machine Learning Models**
- Wav2Vec 2.0 (Speech Emotion Recognition)
- BERT (Text Emotion Classification)

---

### **Core Libraries**
PyTorch ‚Ä¢ Transformers ‚Ä¢ Librosa ‚Ä¢ NLTK ‚Ä¢ Pandas ‚Ä¢ NumPy

---

### **Speech & Audio**
SpeechRecognition ‚Ä¢ PyAudio ‚Ä¢ gTTS ‚Ä¢ pyttsx3

---

### **Visualisation & Evaluation**
Matplotlib ‚Ä¢ Seaborn

---

## ‚öôÔ∏è System Workflow

1Ô∏è‚É£ User provides **speech or text input**  
2Ô∏è‚É£ Speech ‚Üí Wav2Vec 2.0  
3Ô∏è‚É£ Text / Transcript ‚Üí BERT  
4Ô∏è‚É£ Predictions fused via **confidence weighting**  
5Ô∏è‚É£ DSW updates emotional context  
6Ô∏è‚É£ Low-confidence ‚Üí correction logic  
7Ô∏è‚É£ Final emotion ‚Üí OpenChat prompt  
8Ô∏è‚É£ AI response generated  
9Ô∏è‚É£ Output vocalised via **TTS modulation**

---

## ‚ñ∂Ô∏è Running the Project

### **1Ô∏è‚É£ Install Dependencies**

```bash
pip install -r requirements.txt
